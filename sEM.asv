function GMM = sEM(data,numClusters,Options)
    
    arguments 

        data (:,:) double {mustBeReal, mustBeFinite}
        numClusters (1,1) double {mustBeInteger, mustBePositive,...
                                  mustBeNonempty,mustBeNonzero,mustBeNonmissing}
        Options.MaxIterations (1,1) double {mustBeInteger, mustBePositive} = 100;
        Options.NumReplicates (1,1) double {mustBeInteger, mustBePositive} = 5;
        Options.Tolerance (1,1) double {mustBeReal, mustBeFinite} = 1e-6;
        Options.Alpha     (1,1) float  {mustBeInRange(Options.Alpha,0.5,1),...
                                        mustBePositive, mustBeFloat} = 0.5
        Options.BatchSize     (1,1) double {mustBeInRange(Options.BatchSize, ...
                                            1,10000),mustBePositive,... 
                                            mustBeInteger} = 10000
                                        
    end

    [numPoints, dimFeatures] = size(data);
       
    k = 0;
    numIterations = Options.MaxIterations;
    a = Options.Alpha;
    convergenceThreshold = Options.Tolerance; % Set a threshold for convergence

    % Partition the data into batches
    m = Options.BatchSize; % Batch size
    numBatches = ceil(numPoints / m);
    
    % Calculate the number of points after padding
    numPointsPadded = numBatches * m;
    
    % Initialize log-likelihood
    oldLogLikelihood = -inf;

    % Preallocate log_lh
    Log_Likelihood = gpuArray.zeros(numPoints, numClusters);

    % Compute constant term
    constTerm = numDims*log(2*pi)/2;
    
    weights = gpuArray(ones(1, numClusters) / numClusters); % Equal weights
    warning("off")
    [~, mus] = kmeans(gpuArray(data), numClusters, 'MaxIter',numIterations); % Initialize means using kmeans
    Sigmas = gpuArray(ones(numClusters, numDims)); % Unit variances
    
    for j = 1:numClusters


        log_prior = log(weights(j));

        logDetSigma = sum(log(Sigmas(j, :)));

        L = sqrt(Sigmas(j, :));

        Log_Likelihood(:,j) = sum(bsxfun(@rdivide, bsxfun(@minus, ...
            data, mus(j,:)), L).^2, 2);

        Log_Likelihood(:,j) = -0.5*(Log_Likelihood(:,j) + logDetSigma);

        Log_Likelihood(:,j) = Log_Likelihood(:,j) + log_prior - constTerm;

    end

    MaxLogLikelihood = max(Log_Likelihood,[],2);
    responsibilities = exp(Log_Likelihood-MaxLogLikelihood);
    
    for i = 1 : numIterations
    
        fprintf("Now in iteration %d\n",i);
    
        % Create a random order in the data
        randomOrder = gpuArray([randperm(numPoints), 1:(numPointsPadded - numPoints)]);
    
        % Create batches of indices
        batches = reshape(randomOrder(1:numPointsPadded), m, numBatches);
    
        % Create a random order for the batches
        batchOrder = gpuArray(randperm(numBatches));
    
        for j = 1 : numBatches
    
            eta_k = (k+2)^(-a);
    
            batchIndices = unique(batches(:, batchOrder(j)));
            
            % Get the current batch of responsibilities
            currentBatch = Responsibilities(batchIndices,:);
    
            % Apply one-hot encoding 
            one_hot = one_hot_encoding(currentBatch);
    
            % Calculate the NEW responsibilities - Inference
            s_i = currentBatch .* one_hot;
            Responsibilities(batchIndices, :) = (1-eta_k)*Responsibilities(batchIndices, :) + (eta_k * s_i);
            k = k + 1;
        end
        
        % Calculate log-likelihood after updating all batches
        Density = sum(Responsibilities, 2);
        Logpdf = log(Density);
        LogLikelihood = sum(Logpdf);
        
        hold on
        plot(i,LogLikelihood,"o"); 
        xlabel('Iterations'); ylabel('Log-Likelihood'); 
        titleStr = sprintf('Log-Likelihood vs Iterations for \\alpha = %.2f and batch size = %d', a, m);
        title(titleStr);
        drawnow
        
        % Check for convergence
        if abs(LogLikelihood - oldLogLikelihood) < convergenceThreshold
            fprintf('Converged in %d iterations\n', i);
            break;
        end
        oldLogLikelihood = LogLikelihood;
    end
    iter_time = toc
    
    function one_hot = one_hot_encoding(Responsibilities)
        % Get the number of classes
        numClasses = size(Responsibilities, 2);
    
        % For each data point, find the class with the highest responsibility
        [~, maxClass] = max(Responsibilities, [], 2);
        
        % Initialize the one-hot encoded matrix
        one_hot = gpuArray(zeros(size(Responsibilities, 1),numClasses));
        
        % Set the corresponding class to 1 using linear indexing
        one_hot(sub2ind(size(one_hot), (1:size(Responsibilities, 1))', maxClass)) = 1;
    end

end
